---
title: "STAT547O - Backfitting notes"
author: "Matias Salibian-Barrera"
date: "`r format(Sys.Date())`"
output: github_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=5, fig.height=5, 
                      message=FALSE, warning=FALSE)
```

#### LICENSE
These notes are released under the 
"Creative Commons Attribution-ShareAlike 4.0 International" license. 
See the **human-readable version** [here](https://creativecommons.org/licenses/by-sa/4.0/)
and the **real thing** [here](https://creativecommons.org/licenses/by-sa/4.0/legalcode). 


# DRAFT (Read at your own risk)

## Backfitting (robust and otherwise) "by hand"

We use the Air Quality data.
To simplify the example, we will only use two explanatory variables
(`Wind` and `Temp`):
```{r intro}
library(RBF)
data(airquality)
x <- airquality
x <- x[ complete.cases(x), ]
x <- x[, c('Ozone', 'Solar.R', 'Wind', 'Temp')]
y <- as.vector(x$Ozone)
x <- as.matrix(x[, c('Solar.R', 'Wind', 'Temp')])
```
A scatter plot of the data
```{r scatter, fig.width=7, fig.height=7}
pairs(cbind(y,x), labels=c('Ozone', colnames(x)), pch=19, col='gray30', cex=1.5)
```

### Classical backfitting

The algorithm starts with the estimated intercept (equal to the 
sample mean of the response), and all the components of the additive model
set to zero:
```{r initialize.bf}
alpha.hat <- mean(y)
n <- length(y)
bandw <- 5
f.hat.1 <- f.hat.2 <- f.hat.3 <- rep(0, n)
```
We now compute *partial residuals* without using `f.hat.1`:
```{r partial.1}
r.1 <- y - alpha.hat - f.hat.2 - f.hat.3
```
We smooth this vector of residuals as a function of `x1`.
We will use a bandwidth of `span = .65`, this was 
chosen subjectively, essentially just 
by "eyeballing" the plots. It seems to work fine. 
```{r smooth.1}
oo <- order(x[,1])
f.hat.1 <- fitted( loess(r.1 ~ x[,1], span=.65, family='gaussian') ) 
plot(r.1 ~ x[,1], type='p', pch=19, col='gray30')
lines(f.hat.1[oo] ~ x[oo,1], col='blue', lwd=3)
```

Now, compute partial residuals without `f.hat.2` and smooth them 
as a function of `x2`, 
```{r smooth.2}
oo2 <- order(x[,2])
r.2 <- y - alpha.hat - f.hat.1 - f.hat.3
f.hat.2 <- fitted( loess(r.2 ~ x[,2], span=.65, family='gaussian') ) 
plot(r.2 ~ x[,2], type='p', pch=19, col='gray30')
lines(f.hat.2[oo2] ~ x[oo2,2], col='blue', lwd=3)
```

Finally, update `f.hat.3`:
```{r smooth.3}
oo3 <- order(x[,3])
r.3 <- y - alpha.hat - f.hat.1 - f.hat.2
f.hat.3 <- fitted( loess(r.3 ~ x[,3], span=.65, family='gaussian') ) 
plot(r.3 ~ x[,3], type='p', pch=19, col='gray30')
lines(f.hat.3[oo3] ~ x[oo3,3], col='blue', lwd=3)
```

Now perform 15 iterations (why 15? just because I thought they would
be sufficient to converge). Just in case, below we also print the 
approximated L2 norm of consecutive estimates, the
square root of 
$$\sum_{j=1}^3 \| \hat{f}_j^{(k+1)} - \hat{f}_j^{(k)} \|^2$$. 
```{r iterate}
f.hat.3 <- f.hat.3 - mean(f.hat.3)
f.hat.2 <- f.hat.2 - mean(f.hat.2)
f.hat.1 <- f.hat.1 - mean(f.hat.1)
f.hat.1.orig <- f.hat.1
f.hat.2.orig <- f.hat.2
f.hat.3.orig <- f.hat.3

for(i in 1:15) {
  f.hat.1.old <- f.hat.1
  f.hat.2.old <- f.hat.2
  f.hat.3.old <- f.hat.3
  
  r.1 <- y - alpha.hat - f.hat.2 - f.hat.3
  f.hat.1 <- fitted( loess(r.1 ~ x[,1], span=.65, family='gaussian') ) 

  r.2 <- y - alpha.hat - f.hat.1 - f.hat.3
  f.hat.2 <- fitted( loess(r.2 ~ x[,2], span=.65, family='gaussian') ) 

  r.3 <- y - alpha.hat - f.hat.1 - f.hat.2
  f.hat.3 <- fitted( loess(r.3 ~ x[,3], span=.65, family='gaussian') ) 

  f.hat.3 <- f.hat.3 - mean(f.hat.3)
  f.hat.2 <- f.hat.2 - mean(f.hat.2)
  f.hat.1 <- f.hat.1 - mean(f.hat.1)
  
  print(sqrt(mean((f.hat.1-f.hat.1.old)^2) + 
          mean((f.hat.2-f.hat.2.old)^2) +
          mean((f.hat.3-f.hat.3.old)^2)))
}
```
Now plot the "final" estimates, and compare them with the
initial ones:
```{r iterate.show}
plot(r.1 ~ x[,1], type='p', pch=19, col='gray30', main='L2')
lines(f.hat.1[oo] ~ x[oo,1], col='red', lwd=3)
lines(f.hat.1.orig[oo] ~ x[oo,1], col='blue', lwd=3)
legend('topleft', legend=c('Start', 'End'), lwd=3, col=c('blue', 'red'))

plot(r.2 ~ x[,2], type='p', pch=19, col='gray30', main='L2')
lines(f.hat.2[oo2] ~ x[oo2,2], col='red', lwd=3)
lines(f.hat.2.orig[oo2] ~ x[oo2,2], col='blue', lwd=3)
legend('topright', legend=c('Start', 'End'), lwd=3, col=c('blue', 'red'))

plot(r.3 ~ x[,3], type='p', pch=19, col='gray30', main='L2')
lines(f.hat.3[oo3] ~ x[oo3,3], col='red', lwd=3)
lines(f.hat.3.orig[oo3] ~ x[oo3,3], col='blue', lwd=3)
legend('topleft', legend=c('Start', 'End'), lwd=3, col=c('blue', 'red'))
```

#### Sanity check
To verify that our algorithm produces reasonable
results, we compare our "home made" estimates
with those computed with `gam::gam()` (both 
graphically and we look at their values). They 
are of course not identical, but reassuringly close.
```{r trygam, message=FALSE, warning=FALSE}
library(gam)
dat <- as.data.frame(x)
dat$Ozone <- y
gg <- predict(gam(Ozone ~ lo(Solar.R, span=.65) +
                  lo(Wind, span=.65) + lo(Temp, span=.65), data=dat),
              type='terms')

plot(r.1 ~ x[,1], type='p', pch=19, col='gray30')
lines(f.hat.1[oo] ~ x[oo,1], col='red', lwd=3)
lines(gg[oo,1] ~ x[oo,1], col='blue', lwd=3)
legend('topleft', legend=c('gam::gam', 'Home made'), lwd=3, col=c('blue', 'red'))

plot(r.2 ~ x[,2], type='p', pch=19, col='gray30')
lines(f.hat.2[oo2] ~ x[oo2,2], col='red', lwd=3)
lines(gg[oo2,2] ~ x[oo2,2], col='blue', lwd=3)
legend('topright', legend=c('gam::gam', 'Home made'), lwd=3, col=c('blue', 'red'))

plot(r.3 ~ x[,3], type='p', pch=19, col='gray30')
lines(f.hat.3[oo3] ~ x[oo3,3], col='red', lwd=3)
lines(gg[oo3,3] ~ x[oo3,3], col='blue', lwd=3)
legend('topleft', legend=c('gam::gam', 'Home made'), lwd=3, col=c('blue', 'red'))

f.hat.1.cl <- f.hat.1
f.hat.2.cl <- f.hat.2
f.hat.3.cl <- f.hat.3
```

### Robust BF
We now compute robust estimators.  
We first estimate `sigma` (the scale of the 
errors), and we will keep it fixed.
The function `RBF::backf.rob` does this, using a
local median fit to obtain residuals. 
```{r rob1}
library(RBF)
bandw <- c(137, 9, 8)
si.hat <- backf.rob(Xp=x, yp=y, windows=bandw)$sigma.hat
```
The bandwidths above were computed using 
robust cross validation and `RBF::backf.rob`. 
We now need a robust alternative to `loess` (specifically, of 
`predict( loess(...) )`). We will write our own function 
to do this. 
The following function `localM` computes a local M estimator
using a polynomial of 2nd degree. Formally, given 
the data (in the vectors `x` and `y`), the bandwidth
`h`, an estimate of the residual scale `sigma`, and the
choice of tuning parameters for `rho` (in this case we
use Tukey's bisquare function), it computes the solution `a`
to 
```
mean( \rhoprime( (y-a)/sigma), cc=cc) * kernel((x-x_0)/h) = 0
```
By default, `cc` is chosen using the 95% efficiency criterion
for linear regression with Gaussian errors. There are two
additional parameters to control the convergence of the 
weighted least squares iterations. The algorithm is initialized
using a (kernel)-weighted ("local") L1 estimator of regression
(also using a 2nd degree polynomial). 
```{r funct}
localM <- function(x0, x, y, sigma, 
                   cc=RobStatTM::lmrobdet.control(family='bisquare',
                                                  efficiency=.95)$tuning.psi, 
                   h, tol=1e-5, max.it=100) {
  ker.we <- RBF::k.epan((x - x0)/h)
  beta <- coef( quantreg::rq(y ~ I(x-x0) + I((x-x0)^2), weights=ker.we))
  n <- length(y)
  zz <- cbind(rep(1,n), x - x0, (x-x0)^2)
  err <- 10*tol
  j <- 0
  while( (err > tol) & (j < max.it) ) {
    beta.old <- beta
    re <- as.vector(y - zz %*% beta)
    ww1 <- RobStatTM::rhoprime(re/sigma, family='bisquare', cc=cc) / (re/sigma)
    ww1[ is.nan(ww1) ] <- 1
    ww <- ker.we * ww1
    beta <- solve( t(zz) %*% (zz*ww), t(zz * ww) %*% y) 
    err <- sqrt( sum( (beta - beta.old)^2 ) )
    j <- j + 1
  }
  return(beta[1])
}
```
Using this function, we run the first 
step of the backfitting algorithm, exactly as before,
but replacing `loess` with `localM`. Note that in this
case we need to loop through the values of each 
explanatory variable in the training set
(`loess` does this internally when we call
`predict`). We could do something similar using
`RBF::backf.rob`, but it would not be as  
"educational" as doing it by hand. 
```{r robust.onestep, cache=TRUE}
alpha.hat <- RobStatTM::locScaleM(x=y, psi='bisquare')$mu
n <- length(y)
f.hat.1 <- f.hat.2 <- f.hat.3 <- rep(0, n)

r.1 <- y - alpha.hat - f.hat.2 - f.hat.3
oo <- order(x[,1])
for(i in 1:n) 
  f.hat.1[i] <- localM(x0=x[i,1], x=x[,1], y=r.1, sigma=si.hat, h=bandw[1])

plot(r.1 ~ x[,1], type='p', pch=19, col='gray30')
lines(f.hat.1[oo] ~ x[oo,1], col='seagreen', lwd=3)

oo2 <- order(x[,2])
r.2 <- y - alpha.hat - f.hat.1 - f.hat.3
for(i in 1:n) 
  f.hat.2[i] <- localM(x0=x[i,2], x=x[,2], y=r.2, sigma=si.hat, h=bandw[2])

plot(r.2 ~ x[,2], type='p', pch=19, col='gray30')
lines(f.hat.2[oo2] ~ x[oo2,2], col='seagreen', lwd=3)
 
oo3 <- order(x[,3])
r.3 <- y - alpha.hat - f.hat.1 - f.hat.2
for(i in 1:n) 
  f.hat.3[i] <- localM(x0=x[i,3], x=x[,3], y=r.3, sigma=si.hat, h=bandw[3])
plot(r.3 ~ x[,3], type='p', pch=19, col='gray30')
lines(f.hat.3[oo3] ~ x[oo3,3], col='seagreen', lwd=3)
```

Robust iterations
```{r robiter, cache=TRUE}
f.hat.3 <- f.hat.3 - mean(f.hat.3)
f.hat.2 <- f.hat.2 - mean(f.hat.2)
f.hat.1 <- f.hat.1 - mean(f.hat.1)
alpha.hat <- RobStatTM::locScaleM(x=y - f.hat.1 - f.hat.2 - f.hat.3,
                                  psi='bisquare')$mu
f.hat.1.orig <- f.hat.1
f.hat.2.orig <- f.hat.2
f.hat.3.orig <- f.hat.3

for(i in 1:15) {
  f.hat.1.old <- f.hat.1
  f.hat.2.old <- f.hat.2
  f.hat.3.old <- f.hat.3
  
  r.1 <- y - alpha.hat - f.hat.2 - f.hat.3
  for(i in 1:n) 
    f.hat.1[i] <- localM(x0=x[i,1], x=x[,1], y=r.1, sigma=si.hat, h=bandw[1])
  
  r.2 <- y - alpha.hat - f.hat.1 - f.hat.3
  for(i in 1:n) 
    f.hat.2[i] <- localM(x0=x[i,2], x=x[,2], y=r.2, sigma=si.hat, h=bandw[2])
  
  r.3 <- y - alpha.hat - f.hat.1 - f.hat.2
  for(i in 1:n) 
    f.hat.3[i] <- localM(x0=x[i,3], x=x[,3], y=r.3, sigma=si.hat, h=bandw[3])
  
  f.hat.3 <- f.hat.3 - mean(f.hat.3)
  f.hat.2 <- f.hat.2 - mean(f.hat.2)
  f.hat.1 <- f.hat.1 - mean(f.hat.1)
  
  alpha.hat <- RobStatTM::locScaleM(x=y - f.hat.1 - f.hat.2 - f.hat.3,
                                  psi='bisquare')$mu
  
  print(sqrt(mean((f.hat.1-f.hat.1.old)^2) + 
               mean((f.hat.2-f.hat.2.old)^2) +
               mean((f.hat.3-f.hat.3.old)^2)))
}

```

Show
```{r robiterplots}
plot(r.1 ~ x[,1], type='p', pch=19, col='gray30')
lines(f.hat.1[oo] ~ x[oo,1], col='red')
lines(f.hat.1.orig[oo] ~ x[oo,1], col='blue')
lines(f.hat.1.cl[oo] ~ x[oo,1], col='magenta')

plot(r.2 ~ x[,2], type='p', pch=19, col='gray30')
lines(f.hat.2[oo2] ~ x[oo2,2], col='red')
lines(f.hat.2.orig[oo2] ~ x[oo2,2], col='blue')
lines(f.hat.2.cl[oo2] ~ x[oo2,2], col='magenta')

plot(r.3 ~ x[,3], type='p', pch=19, col='gray30')
lines(f.hat.3[oo3] ~ x[oo3,3], col='red')
lines(f.hat.3.orig[oo3] ~ x[oo3,3], col='blue')
lines(f.hat.3.cl[oo3] ~ x[oo3,3], col='magenta')
```



