{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAT547O - Backfitting notes\n",
    "================\n",
    "Matias Salibian-Barrera\n",
    "2019-11-11\n",
    "\n",
    "#### LICENSE\n",
    "\n",
    "These notes are released under the “Creative Commons\n",
    "Attribution-ShareAlike 4.0 International” license. See the\n",
    "**human-readable version**\n",
    "[here](https://creativecommons.org/licenses/by-sa/4.0/) and the **real\n",
    "thing**\n",
    "[here](https://creativecommons.org/licenses/by-sa/4.0/legalcode).\n",
    "\n",
    "# DRAFT (Read at your own risk)\n",
    "\n",
    "## Backfitting (robust and otherwise) “by hand”\n",
    "\n",
    "In these notes we apply the backfitting algorithm to estimate the\n",
    "components of an additive model for the Air Quality Data (available in\n",
    "`R` in package `datasets`). The goals are to illustrate in detail the\n",
    "steps of the algorithm for both the classical (“L2”) and robust cases,\n",
    "and also to compare the resulting estimates. The L2 backfitting\n",
    "algorithm with local polynomial smoothers is implemented in `gam::gam()`\n",
    "and the robust version in `RBF::backf.rob`.\n",
    "\n",
    "First we construct the response vector `y` and the “design matrix” `x`\n",
    "that contains the three available explanatory variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data(airquality)\n",
    "x <- airquality\n",
    "x <- x[ complete.cases(x), ]\n",
    "x <- x[, c('Ozone', 'Solar.R', 'Wind', 'Temp')]\n",
    "y <- as.vector(x$Ozone)\n",
    "x <- as.matrix(x[, c('Solar.R', 'Wind', 'Temp')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below show all the pairwise scatterplots, and is a simple\n",
    "exploratory tool (note that nothing out of the ordinary is apparent in\n",
    "the data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs(cbind(y,x), labels=c('Ozone', colnames(x)), pch=19, col='gray30', cex=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical backfitting\n",
    "\n",
    "As we discussed in class, the classical backfitting algorithm is used to\n",
    "estimate the components of an additive regression model. More\n",
    "specifically, the algorithm is an iterative procedure to find a solution\n",
    "to the first-order system of equations for the L2 loss. There are two\n",
    "loops: the inner loop iterates over the components of the additive\n",
    "model, and the outer loop repeats the outer loop until a convergence (or\n",
    "stopping) criterion is satisfied.\n",
    "\n",
    "It is easy to see that in the L2 case the estimated intercept is the\n",
    "mean of the response, so the algorithm starts setting the intercept\n",
    "estimator equal to the sample mean of the vector of responses. In\n",
    "addition, all the components of the additive model are initialized at\n",
    "zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha.hat <- mean(y)\n",
    "n <- length(y)\n",
    "f.hat.1 <- f.hat.2 <- f.hat.3 <- rep(0, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the first step of the inner loop we compute *partial residuals*\n",
    "without using `f.hat.1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.1 <- y - alpha.hat - f.hat.2 - f.hat.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate the function \\[\\hat{f}_1(a) = E(R_1 | X_1 = a)\\] we use a\n",
    "kernel local regression estimator to smooth the vector of partial\n",
    "residuals `r.1` above as a function of `x1`. In what follows we will use\n",
    "the function `loess` to compute a local polynomial regression estimator,\n",
    "with a bandwidth of `span = .65`. The latter was chosen subjectively\n",
    "(essentially by “eyeballing” the plots so that they look reasonable). In\n",
    "practice one would need a principled approach for this\n",
    "(e.g. cross-validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oo <- order(x[,1])\n",
    "f.hat.1 <- fitted( loess(r.1 ~ x[,1], span=.65, family='gaussian') ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the resulting smoother:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(r.1 ~ x[,1], type='p', pch=19, col='gray30')\n",
    "lines(f.hat.1[oo] ~ x[oo,1], col='blue', lwd=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step of the inner loop is to compute partial residuals without\n",
    "`f.hat.2` and smooth them as a function of `x2`. The code below does\n",
    "this and also displays the resulting estimator of the function\n",
    "\\[\\hat{f}_2(a) = E(R_2 | X_2 = a)\\]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oo2 <- order(x[,2])\n",
    "r.2 <- y - alpha.hat - f.hat.1 - f.hat.3\n",
    "f.hat.2 <- fitted( loess(r.2 ~ x[,2], span=.65, family='gaussian') ) \n",
    "plot(r.2 ~ x[,2], type='p', pch=19, col='gray30')\n",
    "lines(f.hat.2[oo2] ~ x[oo2,2], col='blue', lwd=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we repeat the above to compute an estimator for \\[\\hat{f}_3\\]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oo3 <- order(x[,3])\n",
    "r.3 <- y - alpha.hat - f.hat.1 - f.hat.2\n",
    "f.hat.3 <- fitted( loess(r.3 ~ x[,3], span=.65, family='gaussian') ) \n",
    "plot(r.3 ~ x[,3], type='p', pch=19, col='gray30')\n",
    "lines(f.hat.3[oo3] ~ x[oo3,3], col='blue', lwd=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we center our estimated components of the additive model, since\n",
    "they are constrained to satisfy \\[E(f_j(X_j)) = 0\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.hat.3 <- f.hat.3 - mean(f.hat.3)\n",
    "f.hat.2 <- f.hat.2 - mean(f.hat.2)\n",
    "f.hat.1 <- f.hat.1 - mean(f.hat.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will also save these 1st-step estimates to compare them with the final\n",
    "ones below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.hat.1.orig <- f.hat.1\n",
    "f.hat.2.orig <- f.hat.2\n",
    "f.hat.3.orig <- f.hat.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now completed **the first** pass of the inner loop. The next\n",
    "code chunk perform 15 more iterations of this loop. Why 15? Only because\n",
    "I thought they would be enough for convergence. Just in case, I also\n",
    "print the approximated L2 norm of consecutive estimates (the square root\n",
    "of \\[\\sum_{j=1}^3 \\| \\hat{f}_j^{(k+1)} - \\hat{f}_j^{(k)} \\|^2\\]) to\n",
    "check that the algorithm is indeed converging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(i in 1:15) {\n",
    "  f.hat.1.old <- f.hat.1\n",
    "  f.hat.2.old <- f.hat.2\n",
    "  f.hat.3.old <- f.hat.3\n",
    "  \n",
    "  r.1 <- y - alpha.hat - f.hat.2 - f.hat.3\n",
    "  f.hat.1 <- fitted( loess(r.1 ~ x[,1], span=.65, family='gaussian') ) \n",
    "\n",
    "  r.2 <- y - alpha.hat - f.hat.1 - f.hat.3\n",
    "  f.hat.2 <- fitted( loess(r.2 ~ x[,2], span=.65, family='gaussian') ) \n",
    "\n",
    "  r.3 <- y - alpha.hat - f.hat.1 - f.hat.2\n",
    "  f.hat.3 <- fitted( loess(r.3 ~ x[,3], span=.65, family='gaussian') ) \n",
    "\n",
    "  f.hat.3 <- f.hat.3 - mean(f.hat.3)\n",
    "  f.hat.2 <- f.hat.2 - mean(f.hat.2)\n",
    "  f.hat.1 <- f.hat.1 - mean(f.hat.1)\n",
    "  \n",
    "  print(sqrt(mean((f.hat.1-f.hat.1.old)^2) + \n",
    "          mean((f.hat.2-f.hat.2.old)^2) +\n",
    "          mean((f.hat.3-f.hat.3.old)^2)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the algorithm converges well.  \n",
    "We now plot the “final” estimates, and compare them with the initial\n",
    "ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow=c(2,2))\n",
    "plot(r.1 ~ x[,1], type='p', pch=19, col='gray30', main='L2')\n",
    "lines(f.hat.1[oo] ~ x[oo,1], col='red', lwd=3)\n",
    "lines(f.hat.1.orig[oo] ~ x[oo,1], col='blue', lwd=3)\n",
    "legend('topleft', legend=c('Start', 'End'), lwd=3, col=c('blue', 'red'))\n",
    "\n",
    "plot(r.2 ~ x[,2], type='p', pch=19, col='gray30', main='L2')\n",
    "lines(f.hat.2[oo2] ~ x[oo2,2], col='red', lwd=3)\n",
    "lines(f.hat.2.orig[oo2] ~ x[oo2,2], col='blue', lwd=3)\n",
    "legend('topright', legend=c('Start', 'End'), lwd=3, col=c('blue', 'red'))\n",
    "\n",
    "plot(r.3 ~ x[,3], type='p', pch=19, col='gray30', main='L2')\n",
    "lines(f.hat.3[oo3] ~ x[oo3,3], col='red', lwd=3)\n",
    "lines(f.hat.3.orig[oo3] ~ x[oo3,3], col='blue', lwd=3)\n",
    "legend('topleft', legend=c('Start', 'End'), lwd=3, col=c('blue', 'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** that the above comparisons between the initial estimators and\n",
    "the final ones is not completely accurate, as the partial residuals in\n",
    "each plot (the “dots”) actually depend on the estimated regression\n",
    "functions.\n",
    "\n",
    "#### Sanity check\n",
    "\n",
    "To verify that our code above does in fact produce reasonable results,\n",
    "we compare our “home made” estimates with those computed with\n",
    "`gam::gam()` (both graphically and we look at their values). They are of\n",
    "course not identical, but reassuringly close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(gam)\n",
    "dat <- as.data.frame(x)\n",
    "dat$Ozone <- y\n",
    "gg <- predict(gam(Ozone ~ lo(Solar.R, span=.65) +\n",
    "                  lo(Wind, span=.65) + lo(Temp, span=.65), data=dat),\n",
    "              type='terms')\n",
    "par(mfrow=c(2,2))\n",
    "plot(r.1 ~ x[,1], type='p', pch=19, col='gray30')\n",
    "lines(f.hat.1[oo] ~ x[oo,1], col='red', lwd=3)\n",
    "lines(gg[oo,1] ~ x[oo,1], col='blue', lwd=3)\n",
    "legend('topleft', legend=c('gam::gam', 'Home made'), lwd=3, col=c('blue', 'red'))\n",
    "\n",
    "plot(r.2 ~ x[,2], type='p', pch=19, col='gray30')\n",
    "lines(f.hat.2[oo2] ~ x[oo2,2], col='red', lwd=3)\n",
    "lines(gg[oo2,2] ~ x[oo2,2], col='blue', lwd=3)\n",
    "legend('topright', legend=c('gam::gam', 'Home made'), lwd=3, col=c('blue', 'red'))\n",
    "\n",
    "plot(r.3 ~ x[,3], type='p', pch=19, col='gray30')\n",
    "lines(f.hat.3[oo3] ~ x[oo3,3], col='red', lwd=3)\n",
    "lines(gg[oo3,3] ~ x[oo3,3], col='blue', lwd=3)\n",
    "legend('topleft', legend=c('gam::gam', 'Home made'), lwd=3, col=c('blue', 'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will also save our “final” estimators to compare them with the robust\n",
    "ones below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.hat.1.cl <- f.hat.1\n",
    "f.hat.2.cl <- f.hat.2\n",
    "f.hat.3.cl <- f.hat.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust BF\n",
    "\n",
    "We now turn our attention to robust estimators for the components of an\n",
    "additive model. As we discussed in class, given a loss function `rho` we\n",
    "need to estimate the functions \\[\\tilde{f}_j(a)\\] that solve\n",
    "\\[E( \\rho'((R_j - \\hat{f}_j(a))/\\sigma) \n",
    "| X_j = a) = 0\\], which are the first order conditions of the\n",
    "corresponding “robust” optimization problem. Note that now the intercept\n",
    "estimator solves \\[E( \\rho'((R_j - \\hat{\\alpha}))/\\sigma) ) = 0\\], and\n",
    "thus needs to be updated at the end of each iteration of the inner loop\n",
    "(unlike what happened in the L2 case). Also note that we need an\n",
    "estimate for `sigma`, the scale of the errors, which will remain fixed\n",
    "throughout the rest of the computations. We use the function\n",
    "`RBF::backf.rob` that computes such an estimator using a the residuals\n",
    "with respect to a local median fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandw <- c(137, 9, 8)\n",
    "si.hat <- RBF::backf.rob(Xp=x, yp=y, windows=bandw)$sigma.hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bandwidths above were originally computed using robust cross\n",
    "validation (on a 3-dimensional grid) and the implementation of this\n",
    "robust backfitting in `RBF::backf.rob`.\n",
    "\n",
    "We now need a robust alternative to `loess` (specifically, of `predict(\n",
    "loess(...) )`). I wrote our own function (`localM` below) to compute a\n",
    "local M-estimator of regression, using a polynomial of 2nd degree and\n",
    "Tukey’s loss function. Formally, given:\n",
    "\n",
    "  - the data (in the vectors `x` and `y`);\n",
    "  - the bandwidth `h`;\n",
    "  - the point `x0` at which we want to compute \\[\\hat{f}(x_0)\\];\n",
    "  - an estimate of the residual scale `sigma`; and\n",
    "  - the choice of tuning parameters for `rho` (in this case we use\n",
    "    Tukey’s bisquare function),\n",
    "\n",
    "the function `localM` computes the solution `a` to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean( \\rhoprime( (y-a)/sigma), cc=cc) * kernel((x-x_0)/h) = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the tuning parameter `cc` is chosen to achieve 95%\n",
    "asymptotic efficiency for linear regression models with Gaussian errors.\n",
    "\n",
    "Our function `localM` also accepts two additional parameters (`tol` and\n",
    "`max.it`) to control the convergence of the weighted least squares\n",
    "iterations. The algorithm is initialized using a (kernel) weighted\n",
    "(“local”) L1 estimator of regression (also using a 2nd degree\n",
    "polynomial). The function is here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localM <- function(x0, x, y, sigma, \n",
    "                   cc=RobStatTM::lmrobdet.control(family='bisquare',\n",
    "                                                  efficiency=.95)$tuning.psi, \n",
    "                   h, tol=1e-5, max.it=100) {\n",
    "  ker.we <- RBF::k.epan((x - x0)/h)\n",
    "  beta <- coef( quantreg::rq(y ~ I(x-x0) + I((x-x0)^2), weights=ker.we))\n",
    "  n <- length(y)\n",
    "  zz <- cbind(rep(1,n), x - x0, (x-x0)^2)\n",
    "  err <- 10*tol\n",
    "  j <- 0\n",
    "  while( (err > tol) & (j < max.it) ) {\n",
    "    beta.old <- beta\n",
    "    re <- as.vector(y - zz %*% beta)\n",
    "    ww1 <- RobStatTM::rhoprime(re/sigma, family='bisquare', cc=cc) / (re/sigma)\n",
    "    ww1[ is.nan(ww1) ] <- 1\n",
    "    ww <- ker.we * ww1\n",
    "    beta <- solve( t(zz) %*% (zz*ww), t(zz * ww) %*% y) \n",
    "    err <- sqrt( sum( (beta - beta.old)^2 ) )\n",
    "    j <- j + 1\n",
    "  }\n",
    "  return(beta[1])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run the first step of the backfitting algorithm, exactly as\n",
    "before, but replacing `predict(loess(...))` with `localM(...)`. Note,\n",
    "however, that in this case we need to loop through the values of each\n",
    "explanatory variable in the training set to compute the vector `f.hat.1`\n",
    "of fitted values for the first component estimate (`loess` does this\n",
    "internally when we call `predict`). We could have used the function\n",
    "`RBF::backf.rob` that does this, but the objective of these notes is\n",
    "show every step of the algorithm “by hand”.\n",
    "\n",
    "Here is the first pass of the inner loop. The additive model components\n",
    "are initialized at zero, and the intercept is estimated using the\n",
    "function `RobStaTM::locScaleM` which computes an M-estimator of\n",
    "location. **Note** that this is in fact only an approximation to what we\n",
    "need, since `locScaleM` computes its own residual scale estimator.\n",
    "Although it would be more appropriate to write our own function to\n",
    "compute our intercept estimator, this is not really the objective of\n",
    "these notes. In fact, we leave this task to the reader as a very good\n",
    "exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha.hat <- RobStatTM::locScaleM(x=y, psi='bisquare')$mu\n",
    "n <- length(y)\n",
    "f.hat.1 <- f.hat.2 <- f.hat.3 <- rep(0, n)\n",
    "\n",
    "r.1 <- y - alpha.hat - f.hat.2 - f.hat.3\n",
    "oo <- order(x[,1])\n",
    "for(i in 1:n) \n",
    "  f.hat.1[i] <- localM(x0=x[i,1], x=x[,1], y=r.1, sigma=si.hat, h=bandw[1])\n",
    "\n",
    "plot(r.1 ~ x[,1], type='p', pch=19, col='gray30')\n",
    "lines(f.hat.1[oo] ~ x[oo,1], col='seagreen', lwd=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oo2 <- order(x[,2])\n",
    "r.2 <- y - alpha.hat - f.hat.1 - f.hat.3\n",
    "for(i in 1:n) \n",
    "  f.hat.2[i] <- localM(x0=x[i,2], x=x[,2], y=r.2, sigma=si.hat, h=bandw[2])\n",
    "\n",
    "plot(r.2 ~ x[,2], type='p', pch=19, col='gray30')\n",
    "lines(f.hat.2[oo2] ~ x[oo2,2], col='seagreen', lwd=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oo3 <- order(x[,3])\n",
    "r.3 <- y - alpha.hat - f.hat.1 - f.hat.2\n",
    "for(i in 1:n) \n",
    "  f.hat.3[i] <- localM(x0=x[i,3], x=x[,3], y=r.3, sigma=si.hat, h=bandw[3])\n",
    "plot(r.3 ~ x[,3], type='p', pch=19, col='gray30')\n",
    "lines(f.hat.3[oo3] ~ x[oo3,3], col='seagreen', lwd=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we center the estimated components of the model, and now also\n",
    "update the intercept estimator (we also save this *1st step* estimator\n",
    "for later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.hat.3 <- f.hat.3 - mean(f.hat.3)\n",
    "f.hat.2 <- f.hat.2 - mean(f.hat.2)\n",
    "f.hat.1 <- f.hat.1 - mean(f.hat.1)\n",
    "alpha.hat <- RobStatTM::locScaleM(x=y - f.hat.1 - f.hat.2 - f.hat.3,\n",
    "                                  psi='bisquare')$mu\n",
    "f.hat.1.orig <- f.hat.1\n",
    "f.hat.2.orig <- f.hat.2\n",
    "f.hat.3.orig <- f.hat.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 15 iterations and display a possible convergence criterion to\n",
    "monitor the iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(i in 1:15) {\n",
    "  f.hat.1.old <- f.hat.1\n",
    "  f.hat.2.old <- f.hat.2\n",
    "  f.hat.3.old <- f.hat.3\n",
    "  \n",
    "  r.1 <- y - alpha.hat - f.hat.2 - f.hat.3\n",
    "  for(i in 1:n) \n",
    "    f.hat.1[i] <- localM(x0=x[i,1], x=x[,1], y=r.1, sigma=si.hat, h=bandw[1])\n",
    "  \n",
    "  r.2 <- y - alpha.hat - f.hat.1 - f.hat.3\n",
    "  for(i in 1:n) \n",
    "    f.hat.2[i] <- localM(x0=x[i,2], x=x[,2], y=r.2, sigma=si.hat, h=bandw[2])\n",
    "  \n",
    "  r.3 <- y - alpha.hat - f.hat.1 - f.hat.2\n",
    "  for(i in 1:n) \n",
    "    f.hat.3[i] <- localM(x0=x[i,3], x=x[,3], y=r.3, sigma=si.hat, h=bandw[3])\n",
    "  \n",
    "  f.hat.3 <- f.hat.3 - mean(f.hat.3)\n",
    "  f.hat.2 <- f.hat.2 - mean(f.hat.2)\n",
    "  f.hat.1 <- f.hat.1 - mean(f.hat.1)\n",
    "  \n",
    "  alpha.hat <- RobStatTM::locScaleM(x=y - f.hat.1 - f.hat.2 - f.hat.3,\n",
    "                                  psi='bisquare')$mu\n",
    "  \n",
    "  print(sqrt(mean((f.hat.1-f.hat.1.old)^2) + \n",
    "               mean((f.hat.2-f.hat.2.old)^2) +\n",
    "               mean((f.hat.3-f.hat.3.old)^2)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at the robust estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(r.1 ~ x[,1], type='p', pch=19, col='gray30', main='Robust')\n",
    "lines(f.hat.1[oo] ~ x[oo,1], col='green', lwd=3)\n",
    "lines(f.hat.1.orig[oo] ~ x[oo,1], col='seagreen', lwd=3)\n",
    "lines(f.hat.1.cl[oo] ~ x[oo,1], col='magenta', lwd=3)\n",
    "legend('topleft', legend=c('Robust Final', 'Robust Initial', 'L2 final'), \n",
    "       lwd=3, col=c('green', 'seagreen', 'magenta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(r.2 ~ x[,2], type='p', pch=19, col='gray30', main='Robust')\n",
    "lines(f.hat.2[oo2] ~ x[oo2,2], col='green', lwd=3)\n",
    "lines(f.hat.2.orig[oo2] ~ x[oo2,2], col='seagreen', lwd=3)\n",
    "lines(f.hat.2.cl[oo2] ~ x[oo2,2], col='magenta', lwd=3)\n",
    "legend('topright', legend=c('Robust Final', 'Robust Initial', 'L2 final'), \n",
    "       lwd=3, col=c('green', 'seagreen', 'magenta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(r.3 ~ x[,3], type='p', pch=19, col='gray30', main='Robust')\n",
    "lines(f.hat.3[oo3] ~ x[oo3,3], col='green', lwd=3)\n",
    "lines(f.hat.3.orig[oo3] ~ x[oo3,3], col='seagreen', lwd=3)\n",
    "lines(f.hat.3.cl[oo3] ~ x[oo3,3], col='magenta', lwd=3)\n",
    "legend('topleft', legend=c('Robust Final', 'Robust Initial', 'L2 final'), \n",
    "       lwd=3, col=c('green', 'seagreen', 'magenta'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** that the above comparisons between the two estimators should be\n",
    "used only as an approximation, as the partial residuals in each plot\n",
    "(the “dots”) actually depend on the estimated regression functions.\n",
    "\n",
    "#### Which one is “right” or “better”?\n",
    "\n",
    "Suggestion: compare predictions\\!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
