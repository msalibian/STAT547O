---
title: "STAT547O - Lecture 2 notes"
author: "Matias Salibian-Barrera"
date: "`r format(Sys.Date())`"
output: github_document
---
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=5, fig.height=5, 
message=FALSE, warning=FALSE)
```

#### LICENSE
These notes are released under the 
"Creative Commons Attribution-ShareAlike 4.0 International" license. 
See the **human-readable version** [here](https://creativecommons.org/licenses/by-sa/4.0/)
and the **real thing** [here](https://creativecommons.org/licenses/by-sa/4.0/legalcode). 

# Linear Regression 

In this section we discuss briefly a class of robust estimators 
for linear regression models (re-descending M-estimators). This
class is known to have good robustness properties 
(e.g. high breakdown-point) and can be tuned to be 
highly-efficient when the errors follow a specific distribution. 
Moreover, the score / loss function can be chosen to improve their
asymptotic bias. The main difficulty one encounters with these
estimators is computational, since they require to find the 
minimum of a non-convex function in several dimensions. Much effort
has been put in developing good algorithms, and two alternatives
will be mentioned below. 

## M-estimators

M-estimators for linear regression are the natural extension 
of M-estimators for location/scale models. They can be 
intuitively motivated in a similar way as those for the 
location / scale model (start with a Gaussian MLE estimator
and truncate the loss / score function). Such a monotone score function
(corresponding to a convex loss function, but one that grows 
at a slower rate than the squared loss) was first proposed by 
Huber (1964, 1967, 1981). The corresponding regression 
estimators have bounded influence function, but may have 
a very low breakdown point (as low as $$1/p$$, where $$p$$
is the number of features) if high-leverale outliers are
present. A solution is to use a bounded loss function, 
which results in a re-descending score function&emdash;that
is a score function $$\psi(t)$$ that is zero for 
$$|t| > c$$ for some $$c > 0$$. 
Note that bounded loss functions are necessarily non-convex,
and that the optimization problem that defines these estimators 
may have several critical points that do not correspond to the 
minimum. Computating these estimators can be challenging. 

## The issue of scale

An often overlooked problem is that in order to use these estimators
in practice we need to estimate the scale (standard deviation, if
second moments exist) of the residuals (standardized residuals 
have to be used in the estimating equations). Naturally, this issue also 
afects M-estimators for location / scale, but for them it can 
be solved relatively easily by using the MAD of the observations, 
for example. Note that this robust residual scale estimator 
can be computed independently from the M-estimator. In regression models, 
however, there is no simple robust regression estimator that 
could be used to obtain reliable residuals, in order to compute
a preliminary residual scale estimator. In other words, to
compute a robust regression estimator we need a robust residual
scale estimator. But to compute 
a robust residual
scale estimator we need a robust regression estimator (in order
to obtain reliable residuals). S-estimators (below) will
help break this impasse. 

## S-estimators

## M-estimators with a preliminary scale

#### Computational challenges


#### Choosing the score / loss function









