---
title: "STAT547O - Lecture notes"
author: "Matias Salibian-Barrera"
date: "`r format(Sys.Date())`"
output: github_document
---
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=7, fig.height=7, 
message=FALSE, warning=FALSE)
```

#### LICENSE
These notes are released under the 
"Creative Commons Attribution-ShareAlike 4.0 International" license. 
See the **human-readable version** [here](https://creativecommons.org/licenses/by-sa/4.0/)
and the **real thing** [here](https://creativecommons.org/licenses/by-sa/4.0/legalcode). 

# Simple examples of linear and non-parametric regression

### Simple linear regression

We will compute an S-estimator "by hand". We will use 
a loss function in Tukey's family of bisquare functions. 
We compute a tuning constant that yields $$E(\rho(Z)) = 1/2$$,
where $$Z$$ is a N(0,1) random variable. In this way we 
obtain consistency of the M-scale estimator, and maximum 
asymptotic breakdown point. 

The following code uses the function `lmrobdet.control` to
compute the tuning constant (given the family of functions 
and the desired value of $$E(\rho(Z)))$$. 

```{r cycle, fig.width=6, fig.height=6} 
library(RobStatTM)
# we use bisquare, find constant to have E(\rho(Z)) = 1/2
cc <- lmrobdet.control(family='bisquare', bb=.5)$tuning.chi
```
We now check that in fact, this constant works. 
In what follows we compute Tukey's loss function using 
`rho(..., family='bisquare')`, and later we will
use its derivative
`rhoprime(..., family='bisquare')`. These functions
are available in the package `RobStatTM`. 
```{r sanity}
integrate(function(a, family, cc=cc) rho(a, family=family, cc=cc)*dnorm(a), 
          lower=-Inf, upper=+Inf, family='bisquare', cc=cc)$value
```
We will use the `phosphor` data. 
Details can be found using `help(phosphor, package='RobStatTM')`. 
The response is `plant` and, 
to simplify the example, we select only one explanatory variable,
`organic`.
In these notes we will not use the implementation of S-estimators
available in the `robustbase` and `RobStatTM` packages, but 
rather compute them **by hand**. 
In order to illustrate the impact of outliers, we will 
change the position of the real outlier (from the 
right end of the plot, to the left).
```{r show}
data(phosphor, package='robustbase')
library(RobStatTM)
# artificially change the location of the outlier 
# for illustration purposes
phosphor[17, 'organic'] <- 15
plot(plant ~ organic, data=phosphor, pch=19, col='gray50')
```

As discussed in class, S-estimators, can be shown to solve
the first-order optimality conditions of an M-estimator of
regression, computed using the final scale estimator
(the one corresponding to the S-estimator). Although this
result in itself is not useful to compute the S-estimator
directly (because to use it we would need to know the
residual scale associated to the regression estimator
we want to compute), it does suggest an iterative 
weighted least squares scheme. This is what we use
here. First we create the **design matrix** `x` and the 
**response vector** `y`: 
```{r setup.calc}
n <- nrow(phosphor)
xx <- cbind(rep(1, n), phosphor$organic)
y <- phosphor$plant
```
We now find a random start for our iterative algorithm, 
using the fit to 2 randomly chosen observations:
```{r random_start}
set.seed(123)
(ii <- sample(n, 2))
beta <- coef(lm(plant ~ organic, data=phosphor, subset=ii))
```
We start the iterations from this `beta`, and run it 
100 steps (we will *check for convergence* rather informally
by looking at the sequence of M-estimators of residual scale, 
that we save in the vector `sis`). In each step we use
the function `RobStatTM::mscale` to compute the M-estimator
of scale. 
```{r iter}
# start iterations
sis <- vector('numeric', 100)
for(j in 1:100) {
  re <- as.vector(y - xx %*% beta) 
  sis[j] <- si.hat <- mscale(re, tuning.chi=cc, family='bisquare') 
  ww <- rhoprime(re/si.hat, family='bisquare', cc=cc) / (re/si.hat)
  beta <- solve( t(xx) %*% (xx*ww), t(xx * ww) %*% y) 
}
```

"Check" that the algorithm converged:
```{r conv}
plot(sis, pch=19, col='gray30', xlab='Iteration', ylab='Sigma hat')
```

And also
```{r conv2}
sis
```

We now show the S-regression estimator, in red, and the least squares
one, in blue:
```{r aaa}
beta.S <- beta
plot(plant ~ organic, data=phosphor, pch=19, col='gray50')
abline(beta.S, lwd=3, col='tomato3')
abline(lm(plant ~ organic, data=phosphor), lwd=3, col='steelblue3')
legend('topright', lwd=3, lty=1, col=c('tomato3', 'steelblue3'), 
       legend=c('S', 'LS'))
```

Furthermore, compare these estimators with the LS one without 
the outlier:
```{r noout}
plot(plant ~ organic, data=phosphor, pch=19, col='gray50')
abline(beta.S, lwd=3, col='tomato3')
abline(lm(plant ~ organic, data=phosphor), lwd=3, col='steelblue3')
abline(lm(plant ~ organic, data=phosphor, subset=-17), lwd=3, col='green3')
legend('topright', lwd=3, lty=1, col=c('tomato3', 'steelblue3', 'green3'), 
       legend=c('S', 'LS', 'LS(clean)'))
```

We now use the S-estimator as a starting point to compute a more
efficient M-estimator of regression, using residual scale associated
with the S-estimator. The "rho" function is in the same 
family, but the tuning constant changes. We also
use 100 iterations, and note that we do not update the
residual M-scale estimator:
```{r a2}
cc2 <- lmrobdet.control(family='bisquare', bb=.5)$tuning.psi
for(j in 1:100) {
  re <- as.vector(y - xx %*% beta)
  ww <- rhoprime(re/si.hat, family='bisquare', cc=cc2) / (re/si.hat)
  beta <- solve( t(xx) %*% (xx*ww), t(xx * ww) %*% y) 
}
```
We now plot all estimators computed so far:
```{r allplot}
beta.M <- beta
plot(plant ~ organic, data=phosphor, pch=19, col='gray50')
abline(beta.S, lwd=3, col='tomato3')
abline(lm(plant ~ organic, data=phosphor), lwd=3, col='steelblue3')
abline(lm(plant ~ organic, data=phosphor, subset=-17), lwd=3, col='green3')
abline(beta.M, lwd=3, col='magenta3')
legend('topright', lwd=3, lty=1, 
       col=c('tomato3', 'steelblue3', 'green3', 'magenta3'), 
       legend=c('S', 'LS', 'LS(clean)', 'MM'))
```

Note that, unlike the S-estimator, the MM-estimator is indistinguishable from the 
LS estimator computed on the clean data. This is the desired
result of using an efficient and robust estimator. 


<!-- # a2 <- robustbase::lmrob(plant ~ organic, data=phosphor) -->
<!-- # beta2 <- a2$init.S$coef -->
<!-- # re2 <- as.vector(y - xx%*%beta2) -->
<!-- # sum( rho(re2/a2$init.S$scale, family='bisquare', cc=cc) ) / 16 -->
<!-- # sum( rho(re/si.hat, family='bisquare', cc=cc) ) / 16 -->



```{r u}
myc <- lmrobdet.control(family='bisquare', efficiency=0.85)
ph.M <- lmrobM(plant ~ inorg, data=phosphor, control=myc)
plot(plant ~ inorg, data=phosphor, pch=19, cex=1.2)
abline(ph.M, lwd=2, col='tomato3')
legend(5, 160, legend='lmrobM fit', lwd=2, col='tomato3')
```

