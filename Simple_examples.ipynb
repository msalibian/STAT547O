{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAT547O - Lecture notes\n",
    "================\n",
    "Matias Salibian-Barrera\n",
    "2019-11-04\n",
    "\n",
    "#### LICENSE\n",
    "\n",
    "These notes are released under the \"Creative Commons Attribution-ShareAlike 4.0 International\" license. See the **human-readable version** [here](https://creativecommons.org/licenses/by-sa/4.0/) and the **real thing** [here](https://creativecommons.org/licenses/by-sa/4.0/legalcode).\n",
    "\n",
    "Simple examples of linear and non-parametric regression\n",
    "=======================================================\n",
    "\n",
    "### Simple linear regression\n",
    "\n",
    "We will compute an S-estimator \"by hand\". We will use a loss function in Tukey's family of bisquare functions. We compute a tuning constant that yields\n",
    "*E*(*ρ*(*Z*)) = 1/2\n",
    ", where\n",
    "*Z*\n",
    " is a N(0,1) random variable. In this way we obtain consistency of the M-scale estimator, and maximum asymptotic breakdown point.\n",
    "\n",
    "The following code uses the function `lmrobdet.control` to compute the tuning constant (given the family of functions and the desired value of\n",
    "*E*(*ρ*(*Z*)))\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(RobStatTM)\n",
    "# we use bisquare, find constant to have E(\\rho(Z)) = 1/2\n",
    "cc <- lmrobdet.control(family='bisquare', bb=.5)$tuning.chi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check that in fact, this constant works. In what follows we compute Tukey's loss function using `rho(..., family='bisquare')`, and later we will use its derivative `rhoprime(..., family='bisquare')`. These functions are available in the package `RobStatTM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrate(function(a, family, cc=cc) rho(a, family=family, cc=cc)*dnorm(a), \n",
    "          lower=-Inf, upper=+Inf, family='bisquare', cc=cc)$value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `phosphor` data. Details can be found using `help(phosphor, package='RobStatTM')`. The response is `plant` and, to simplify the example, we select only one explanatory variable, `organic`. In these notes we will not use the implementation of S-estimators available in the `robustbase` and `RobStatTM` packages, but rather compute them **by hand**. In order to illustrate the impact of outliers, we will change the position of the real outlier (from the right end of the plot, to the left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data(phosphor, package='robustbase')\n",
    "library(RobStatTM)\n",
    "# artificially change the location of the outlier \n",
    "# for illustration purposes\n",
    "phosphor[17, 'organic'] <- 15\n",
    "plot(plant ~ organic, data=phosphor, pch=19, col='gray50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will discuss in class that S-estimators can be shown to solve the first-order optimality conditions of an M-estimator of regression, computed using the final scale estimator (the one corresponding to the S-estimator). Although this result in itself is not useful to compute the S-estimator directly (because to use it we would need to know the residual scale associated to the regression estimator we want to compute), it does suggest an iterative weighted least squares scheme. This is what we use here. First we create the **design matrix** `x` and the **response vector** `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n <- nrow(phosphor)\n",
    "xx <- cbind(rep(1, n), phosphor$organic)\n",
    "y <- phosphor$plant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now find a random start for our iterative algorithm, using the fit to 2 randomly chosen observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "(ii <- sample(n, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta <- coef(lm(plant ~ organic, data=phosphor, subset=ii))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start the iterations from this `beta`, and run it 100 steps (we will *check for convergence* rather informally by looking at the sequence of M-estimators of residual scale, that we save in the vector `sis`). In each step we use the function `RobStatTM::mscale` to compute the M-estimator of scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start iterations\n",
    "sis <- vector('numeric', 100)\n",
    "for(j in 1:100) {\n",
    "  re <- as.vector(y - xx %*% beta) \n",
    "  sis[j] <- si.hat <- mscale(re, tuning.chi=cc, family='bisquare') \n",
    "  ww <- rhoprime(re/si.hat, family='bisquare', cc=cc) / (re/si.hat)\n",
    "  beta <- solve( t(xx) %*% (xx*ww), t(xx * ww) %*% y) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Check\" that the algorithm converged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(sis, pch=19, col='gray30', xlab='Iteration', ylab='Sigma hat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now show the S-regression estimator, in red, and the least squares one, in blue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta.S <- beta\n",
    "plot(plant ~ organic, data=phosphor, pch=19, col='gray50')\n",
    "abline(beta.S, lwd=3, col='tomato3')\n",
    "abline(lm(plant ~ organic, data=phosphor), lwd=3, col='steelblue3')\n",
    "legend('topright', lwd=3, lty=1, col=c('tomato3', 'steelblue3'), \n",
    "       legend=c('S', 'LS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, compare these estimators with the LS one without the outlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(plant ~ organic, data=phosphor, pch=19, col='gray50')\n",
    "abline(beta.S, lwd=3, col='tomato3')\n",
    "abline(lm(plant ~ organic, data=phosphor), lwd=3, col='steelblue3')\n",
    "abline(lm(plant ~ organic, data=phosphor, subset=-17), lwd=3, col='green3')\n",
    "legend('topright', lwd=3, lty=1, col=c('tomato3', 'steelblue3', 'green3'), \n",
    "       legend=c('S', 'LS', 'LS(clean)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the S-estimator as a starting point to compute a more efficient M-estimator of regression, using residual scale associated with the S-estimator. The \"rho\" function is in the same family, but the tuning constant changes. We also use 100 iterations, and note that we do not update the residual M-scale estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc2 <- lmrobdet.control(family='bisquare', bb=.5)$tuning.psi\n",
    "for(j in 1:100) {\n",
    "  re <- as.vector(y - xx %*% beta)\n",
    "  ww <- rhoprime(re/si.hat, family='bisquare', cc=cc2) / (re/si.hat)\n",
    "  beta <- solve( t(xx) %*% (xx*ww), t(xx * ww) %*% y) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot all estimators computed so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta.M <- beta\n",
    "plot(plant ~ organic, data=phosphor, pch=19, col='gray50')\n",
    "abline(beta.S, lwd=3, col='tomato3')\n",
    "abline(lm(plant ~ organic, data=phosphor), lwd=3, col='steelblue3')\n",
    "abline(lm(plant ~ organic, data=phosphor, subset=-17), lwd=3, col='green3')\n",
    "abline(beta.M, lwd=3, col='magenta3')\n",
    "legend('topright', lwd=3, lty=1, \n",
    "       col=c('tomato3', 'steelblue3', 'green3', 'magenta3'), \n",
    "       legend=c('S', 'LS', 'LS(clean)', 'MM'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, unlike the S-estimator, the MM-estimator is indistinguishable from the LS estimator computed on the clean data. This is the desired result of using an efficient and robust estimator.\n",
    "\n",
    "\n",
    "### Non-parametric regression\n",
    "\n",
    "Consider the motorcycle acceleration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data(mcycle, package='MASS')\n",
    "plot(accel ~ times, data=mcycle, pch=19, col='gray50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute a Kernel M-estimator at `x0 = 17`. We first need an estimator of the residual scale. Looking at the plot, it does not seem reasonable to assume a homogeneous model, so we look for a *local* residual scale estimator. We use a bandwidth `h = 2`. The local MAD (using a local median estimator) is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 <- 17\n",
    "h <- 3\n",
    "si.hat <- with(mcycle, mad( accel[ abs(times-x0) < h] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that residuals with respect to a local L1 estimator would be much smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii <- with(mcycle, which( abs(times-x0) < h) )\n",
    "si.hat2 <- mad( resid( quantreg::rq(accel ~ times, data=mcycle, subset=ii) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute\n",
    "*f̂*(*x*0)\n",
    ". We use the Epanechnikov kernel as implemented in `RBF::k.epan`, and the residual scale estimator `si.hat2` above. We compute a local linear fit, with a redescending\n",
    "*ρ*\n",
    ", from Tukey's bisquare family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial\n",
    "beta <- coef( quantreg::rq(accel ~ I(times-x0), data=mcycle, subset=ii) )\n",
    "n <- nrow(mcycle)\n",
    "zz <- cbind(rep(1,n), mcycle$times - x0)\n",
    "ker.we <- RBF::k.epan((mcycle$times - x0)/h)\n",
    "y <- mcycle$accel\n",
    "cc2 <- lmrobdet.control(family='bisquare', bb=.5)$tuning.psi\n",
    "for(j in 1:100) {\n",
    "  re <- as.vector(y - zz %*% beta)\n",
    "  ww1 <- rhoprime(re/si.hat2, family='bisquare', cc=cc2) / (re/si.hat2)\n",
    "  ww1[ is.nan(ww1) ] <- 1\n",
    "  ww <- ker.we * ww1\n",
    "  beta <- solve( t(zz) %*% (zz*ww), t(zz * ww) %*% y) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(accel ~ times, data=mcycle, pch=19, col='gray50')\n",
    "points(accel ~ times, data=mcycle, pch=19, col='steelblue3', subset=ii)\n",
    "abline(v=17)\n",
    "points(17, beta[1], pch=19, col='magenta', cex=1.3)\n",
    "abline(beta[1] - 17*beta[2], beta[2], lwd=1, col='steelblue3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for\n",
    "*x*0 = 18\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save fit for 17\n",
    "beta17 <- beta\n",
    "x0 <- 18\n",
    "ii <- with(mcycle, which( abs(times-x0) < 3) )\n",
    "si.hat2 <- mad( resid( quantreg::rq(accel ~ times, data=mcycle, subset=ii) ) )\n",
    "beta <- coef( quantreg::rq(accel ~ I(times-x0), data=mcycle, subset=ii) )\n",
    "zz <- cbind(rep(1,n), mcycle$times - x0)\n",
    "ker.we <- RBF::k.epan((mcycle$times - x0)/h)\n",
    "for(j in 1:100) {\n",
    "  re <- as.vector(y - zz %*% beta)\n",
    "  ww1 <- rhoprime(re/si.hat2, family='bisquare', cc=cc2) / (re/si.hat2)\n",
    "  ww1[ is.nan(ww1) ] <- 1\n",
    "  ww <- ker.we * ww1\n",
    "  beta <- solve( t(zz) %*% (zz*ww), t(zz * ww) %*% y) \n",
    "}\n",
    "beta18 <- beta\n",
    "plot(accel ~ times, data=mcycle, pch=19, col='gray50')\n",
    "abline(v=c(17, 18))\n",
    "points(17, beta17[1], pch=19, col='magenta', cex=1.3)\n",
    "points(18, beta18[1], pch=19, col='magenta', cex=1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with the fit from package `RBF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(RBF)\n",
    "tt <- with(mcycle, seq(min(times), max(times), length=200))\n",
    "a <- backf.rob(Xp = mcycle$times, yp=mcycle$accel, windows=h, \n",
    "               point=as.matrix(tt), type='Tukey', degree=1) \n",
    "plot(accel ~ times, data=mcycle, pch=19, col='gray50')\n",
    "lines(tt, a$prediction+a$alpha, col='tomato3', lwd=3)\n",
    "points(17, beta17[1], pch=19, col='magenta', cex=1.3)\n",
    "points(18, beta18[1], pch=19, col='magenta', cex=1.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
